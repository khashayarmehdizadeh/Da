{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas is a powerful and flexible open-source data analysis and manipulation library for Python. It provides data structures like DataFrames and Series, which are essential for handling structured data. With pandas, you can easily perform operations such as data cleaning, transformation, aggregation, and visualization. It is widely used in data science, machine learning, and statistical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd \n",
    "and read data and save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "df =yf.download()\n",
    "df =pd.read_csv(\"data.csv\")#read csv\n",
    "print(df.head())#show 5 line frist of the data\n",
    "df_excel=pd.read_excel('data.xlsx',sheet_name='sheet') #now we read sheet from data xlsx\n",
    "                             #how to save DataFrame?\n",
    "df.to_csv('output.csv',index=False) #save csv whithout index \n",
    "df.to_excel('output.xlsx',sheet_name='Results',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading data in Pandas, it is usually necessary to get an initial understanding of the content and structure of the DataFrame. Here are some useful methods for this purpose:\n",
    "\n",
    "df.head(n) and df.tail(n): Display the first or last n rows (if n is not specified, the default is 5). This provides a quick view of the data.\n",
    "df.info(): Displays a summary of the DataFrame, including the number of rows and columns, column names and data types, and the count of non-null values.\n",
    "df.shape: Returns the dimensions of the DataFrame as (number of rows, number of columns).\n",
    "df.describe(): Provides basic descriptive statistics for numerical columns (such as mean, standard deviation, minimum, maximum, and quartiles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the after read the df\n",
    "print(df.shape) #the exist 5,1000 5 column and 10000 line\n",
    "print(df.columns) #show title of colmun\n",
    "df.info() #show summary of the data\n",
    "print(df.head(3)) # show the 3 line of frist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of these commands provides general information about the DataFrame df. For example, df.info() is useful to determine the data type of each column (integer, float, string, etc.) and to check whether any columns contain missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.describe())\n",
    "# The df.describe() method provides a summary of the numerical columns in the DataFrame.\n",
    "# It includes the count, mean, standard deviation, minimum, 25th percentile, median (50th percentile), 75th percentile, and maximum values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting and Filtering Data\n",
    "Pandas provides powerful capabilities for selecting subsets of data (including specific rows and columns) and filtering based on logical conditions. Below are different methods for selecting data:\n",
    "\n",
    "Selecting Columns\n",
    "\n",
    "Using column names:\n",
    "df['ColumnName'] returns a Series containing the data of that column.\n",
    "To select multiple columns, pass a list of column names: df[['Col1', 'Col2']] returns a DataFrame containing only those columns.\n",
    "\n",
    "Using attributes:\n",
    "\n",
    "If the column name has no spaces or special characters, you can also use df.ColumnName. However, the previous method is more general and reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select one column \n",
    "ages=df['Age'] #The series related to the 'Age' column.\n",
    "\n",
    "#select some colmun\n",
    "\n",
    "subset=df[['Namw','City']] #The DataFrame includes two columns 'Name' and 'City'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting rows based on index using label-based and positional methods:\n",
    "\n",
    " df.loc[index_label] for selection based on the label or name of the row index (and also column), and df.iloc[index_position] for selection based on numerical position (row/column index numbers). \n",
    "\n",
    " To select a range of rows: df[start_index : end_index] (similar to slicing a list in Python) also works and returns rows from start to end-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "first_row=df.iloc[0] #The first row of the DataFrame as a Series.\n",
    "first_three_rows=df.iloc[0:3] #First three rows of the DataFrame.\n",
    "\n",
    "# If the dataframe has label-based indexing (for example, names of individuals) \n",
    "# df.loc['Ali'] returns the row with the label 'Ali' in the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering rows based on a condition\n",
    " is one of the most commonly used features of Pandas; it involves filtering rows based on logical conditions. This is done by creating a boolean mask (True/False) and applying it to the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adults=df[df['Age']>30] #filtering the age more than 30\n",
    "\n",
    "mask=(df['Age']>30)& (df['City'] == 'Tehran')\n",
    "\n",
    "filtered_df=df[mask]\n",
    "print(filtered_df)\n",
    "\n",
    "#Filter with multiple conditions: individuals who are older than 30 and their city is Tehran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, df['Age'] > 30 produces a Boolean Series that indicates for each row whether the condition of age being over 30 is met or not. Using & (logical AND) between two conditions allows for the combination of conditions (note: in Pandas, you should use & instead of and, and the sides of the conditions should be enclosed in parentheses). Finally, df[mask] returns the rows where the condition is True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method for filtering is to use the query method, which is written as a logical condition string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering with query (writing style similar to SQL)\n",
    "filtered_df=df.query(\"Age > and City =='Tehran'\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning and preparing data\n",
    " Real-world data often needs cleaning and preprocessing to be suitable for analysis. Pandas offers many functions for data cleaning, and in this section, we will refer to the most important ones: \n",
    "Missing Values: \n",
    "  Identifying and managing missing or empty data is very important. In Pandas, these values are typically represented as NaN (Not a Number). To find missing values, one can use df.isnull() or df.isna() (and their inverses df.notnull()), which return a boolean DataFrame.\n",
    " Removing missing values: \n",
    "  With df.dropna(), one can delete rows that have missing values. For example, df.dropna(subset=['Age', 'City']) will delete rows that have NaN in the Age or City columns. The parameter how='all' or how='any' can be used to remove rows where all their columns are empty or at least one of their columns is empty (the default is how='any').\n",
    " Replacingmissing values:\n",
    "  With df.fillna(), missing values can be filled with a fixed value or, for example, the mean of that column. Example: df['Age'].fillna(df['Age'].mean(), inplace=True) places the mean age in all empty locations of the Age column (the inplace=True parameter applies the change to the DataFrame itself).\n",
    " Removing duplicate data:\n",
    "  If there are duplicate rows, we use df.drop_duplicates(). This method can operate based on one or more specific columns. For example, df.drop_duplicates(subset=['Name', 'City'], keep='first') removes duplicate rows concerning the combination of Name and City (except for the first occurrence).\n",
    " Changing the data type of columns:\n",
    "  Sometimes it is necessary to change the data type of a column (for example, from string to number or vice versa). The astype method can be used. Example: df['Age'] = df['Age'].astype('int') changes the data type of the Age column to integer. If the conversion is inconsistent (for example, converting letters to numbers), an error occurs, which needs to be cleaned or managed before conversion.\n",
    " Changing column names:\n",
    "  The df.rename() method can be used to change column or index names. Example: df.rename(columns={'OldName': 'NewName'}, inplace=True). Additionally, the df.columns attribute is editable; for instance, df.columns = ['A', 'B', 'C'] changes the names of all columns at once. \n",
    "Applying functions to data:\n",
    " Pandas allows for applying a function to all values of a column or row with apply. For example, if we want to convert the price column, which is in dollars, to tomans (assuming the dollar is 30,000 tomans):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Price_Toman'] = df['Price'].apply(lambda x: x * 30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command adds a new column Price_Toman to the dataframe, which is the result of multiplying the Price value by 30000. The apply method is very flexible and can apply more complex functions (or even user-defined functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping and summarizing data is one of the key capabilities\n",
    " of Pandas, allowing for the grouping of data and then performing aggregations on each group. This functions similarly to the GROUP BY statement in SQL and is very useful for summarizing data based on a common attribute.\n",
    " To group data in Pandas, we use the groupby() method. This method creates a grouping based on one or more columns and returns a special object on which operations such as calculating the mean, sum, count, and so on can be performed.\n",
    "  Example: Suppose we have a DataFrame named sales_df that includes the 'City' and 'Revenue' columns. We want to calculate the total revenue based on each city:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped=sales_df.groupby('City') #grouped by City column\n",
    "result=grouped['Revenue'].sum() # Calculate the total income for each city\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, sales_df.groupby('City') groups the data based on the values in the City column. Then, we selected the 'Revenue' column on the grouped object and applied the sum() method, resulting in a new Series containing the total revenue for each unique City value. The output of print(result) may look something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "City\n",
    "Tehran     1500000  \n",
    "Shiraz      800000  \n",
    "Mashhad     620000  \n",
    "...            ...  \n",
    "Name: Revenue, dtype: int64\n",
    "\n",
    " In the above example, we have the total Revenue for each city. Similarly, one can use other functions such as mean, count, max, min, etc. Moreover, multiple functions can be applied simultaneously using the agg method:\n",
    "It is also possible to use multiple functions simultaneously with the agg method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = grouped['Revenue'].agg(['count','mean','sum'])\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code returns a dataframe that shows the number of rows, average income, and total income for each city.\n",
    " To group by multiple columns, we simply need to pass the list of columns to groupby. For example: df.groupby(['Region', 'City']).sum() groups the data first by Region and then by City, providing the sum for each combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting data:\n",
    " Sorting (Sorting) data either by the values of the columns or by the index is easily done in Pandas: df.sort_values(by='ColumnName'):\n",
    "  sorts the DataFrame in ascending order based on the specified column value (ascending is the default). To sort in descending order, we use ascending=False. You can also provide by as a list of columns to sort based on priority. df.sort_index(): \n",
    "  sorts the DataFrame by index. To sort the index in reverse, ascending=False can be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting by a column in ascending order\n",
    "sorted_df=df.sort_values(by='Age')\n",
    "\n",
    "#Sorting by a column in descending order\n",
    "sorted_df2=df.sort_values(by='Revenue',ascending=False)\n",
    "\n",
    "#Sorting by multiple columns (for example, first alphabetically by City,\n",
    "#and in case of a tie, by Age in ascending order)\n",
    "sorted_df3=df.sort_values(by=['City','Age'],ascending=[True,True])\n",
    "\n",
    "#Sorting by index of lines\n",
    "df_sorted_index=df.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After sorting, the sorted dataframe (sorted_df, etc.) can be viewed or used. If we want the sorting change to be applied to the same dataframe, we can add inplace=True to the method (for example, df.sort_values(..., inplace=True)). However, please note that in this case, the original dataframe will be modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data projects, it is often necessary to combine information from multiple data sources. The Pandas library has two main methods for combining data: merging and concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows (df.head()):\n",
      "Price            Close        High         Low        Open       Volume\n",
      "Ticker         ETH-USD     ETH-USD     ETH-USD     ETH-USD      ETH-USD\n",
      "Date                                                                   \n",
      "2020-01-01  130.802002  132.835358  129.198288  129.630661   7935230330\n",
      "2020-01-02  127.410179  130.820038  126.954910  130.820038   8032709256\n",
      "2020-01-03  134.171707  134.554016  126.490021  127.411263  10476845358\n",
      "2020-01-04  135.069366  136.052719  133.040558  134.168518   7430904515\n",
      "2020-01-05  136.276779  139.410202  135.045624  135.072098   7526675353\n",
      "\n",
      "Last few rows (df.tail()):\n",
      "Price             Close         High          Low         Open       Volume\n",
      "Ticker          ETH-USD      ETH-USD      ETH-USD      ETH-USD      ETH-USD\n",
      "Date                                                                       \n",
      "2021-02-24  1626.575684  1710.983765  1511.018921  1571.476440  31329000537\n",
      "2021-02-25  1475.703735  1670.224121  1465.058960  1625.393921  24481681873\n",
      "2021-02-26  1446.033691  1559.028931  1407.979248  1478.653320  31435997881\n",
      "2021-02-27  1459.973145  1524.932373  1433.786987  1446.929443  20742103233\n",
      "2021-02-28  1416.048950  1468.391479  1300.472168  1459.860474  27637026080\n",
      "\n",
      "DataFrame information (df.info()):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 425 entries, 2020-01-01 to 2021-02-28\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   (Close, ETH-USD)   425 non-null    float64\n",
      " 1   (High, ETH-USD)    425 non-null    float64\n",
      " 2   (Low, ETH-USD)     425 non-null    float64\n",
      " 3   (Open, ETH-USD)    425 non-null    float64\n",
      " 4   (Volume, ETH-USD)  425 non-null    int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 19.9 KB\n",
      "\n",
      "DataFrame shape (df.shape):\n",
      "(425, 5)\n",
      "\n",
      "Descriptive statistics (df.describe()):\n",
      "Price         Close         High          Low         Open        Volume\n",
      "Ticker      ETH-USD      ETH-USD      ETH-USD      ETH-USD       ETH-USD\n",
      "count    425.000000   425.000000   425.000000   425.000000  4.250000e+02\n",
      "mean     464.548156   479.237242   443.109194   461.620421  1.741914e+10\n",
      "std      428.303410   445.941009   401.839576   426.272736  9.909833e+09\n",
      "min      110.605873   116.021622    95.184303   110.406784  5.109033e+09\n",
      "25%      208.863434   214.597717   204.031128   208.716064  1.121195e+10\n",
      "50%      334.586639   342.450836   316.774353   321.829742  1.434619e+10\n",
      "75%      462.960541   475.217255   452.072418   462.959534  2.024132e+10\n",
      "max     1960.164795  2036.286499  1896.684448  1959.902954  6.073363e+10\n",
      "\n",
      "'Open' prices:\n",
      "Ticker         ETH-USD\n",
      "Date                  \n",
      "2020-01-01  129.630661\n",
      "2020-01-02  130.820038\n",
      "2020-01-03  127.411263\n",
      "2020-01-04  134.168518\n",
      "2020-01-05  135.072098\n",
      "\n",
      "'Close' and 'Volume' columns:\n",
      "Price            Close       Volume\n",
      "Ticker         ETH-USD      ETH-USD\n",
      "Date                               \n",
      "2020-01-01  130.802002   7935230330\n",
      "2020-01-02  127.410179   8032709256\n",
      "2020-01-03  134.171707  10476845358\n",
      "2020-01-04  135.069366   7430904515\n",
      "2020-01-05  136.276779   7526675353\n",
      "\n",
      "Rows with 'Close' price above 50000:\n",
      "Price        Close    High     Low    Open  Volume\n",
      "Ticker     ETH-USD ETH-USD ETH-USD ETH-USD ETH-USD\n",
      "Date                                              \n",
      "2020-01-01     NaN     NaN     NaN     NaN     NaN\n",
      "2020-01-02     NaN     NaN     NaN     NaN     NaN\n",
      "2020-01-03     NaN     NaN     NaN     NaN     NaN\n",
      "2020-01-04     NaN     NaN     NaN     NaN     NaN\n",
      "2020-01-05     NaN     NaN     NaN     NaN     NaN\n",
      "\n",
      "Data between 2023-01-01 and 2023-03-01:\n",
      "Empty DataFrame\n",
      "Columns: [(Close, ETH-USD), (High, ETH-USD), (Low, ETH-USD), (Open, ETH-USD), (Volume, ETH-USD)]\n",
      "Index: []\n",
      "\n",
      "First 10 rows using iloc:\n",
      "Price            Close        High         Low        Open       Volume\n",
      "Ticker         ETH-USD     ETH-USD     ETH-USD     ETH-USD      ETH-USD\n",
      "Date                                                                   \n",
      "2020-01-01  130.802002  132.835358  129.198288  129.630661   7935230330\n",
      "2020-01-02  127.410179  130.820038  126.954910  130.820038   8032709256\n",
      "2020-01-03  134.171707  134.554016  126.490021  127.411263  10476845358\n",
      "2020-01-04  135.069366  136.052719  133.040558  134.168518   7430904515\n",
      "2020-01-05  136.276779  139.410202  135.045624  135.072098   7526675353\n",
      "2020-01-06  144.304153  144.328186  136.079636  136.305542   9093747121\n",
      "2020-01-07  143.543991  145.001770  140.488876  144.311996   9257954672\n",
      "2020-01-08  141.258133  146.828003  138.268204  143.487823   9387194957\n",
      "2020-01-09  138.979202  141.393982  136.236404  141.155777   7697325348\n",
      "2020-01-10  143.963776  144.167328  135.520401  138.967041   9084816203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries if not already installed\n",
    "# !pip install yfinance\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Download Bitcoin data from Yahoo Finance\n",
    "df = yf.download(\"ETH-USD\", start=\"2020-01-01\", end=\"2021-03-01\")\n",
    "\n",
    "# 2. Display the first few and last few rows of the DataFrame\n",
    "print(\"First few rows (df.head()):\")\n",
    "print(df.head())  # By default, displays 5 rows\n",
    "\n",
    "print(\"\\nLast few rows (df.tail()):\")\n",
    "print(df.tail())  # By default, displays 5 rows\n",
    "\n",
    "# 3. Display DataFrame summary information\n",
    "print(\"\\nDataFrame information (df.info()):\")\n",
    "df.info()\n",
    "\n",
    "# 4. Display the shape of the DataFrame (number of rows and columns)\n",
    "print(\"\\nDataFrame shape (df.shape):\")\n",
    "print(df.shape)\n",
    "\n",
    "# 5. Display descriptive statistics for numerical columns\n",
    "print(\"\\nDescriptive statistics (df.describe()):\")\n",
    "print(df.describe())\n",
    "\n",
    "# 6. Remove duplicate rows and rows with missing values\n",
    "df = df.drop_duplicates()  # Remove duplicate rows\n",
    "df = df.dropna()           # Remove rows with missing (NaN) values\n",
    "\n",
    "# 7. Sort the DataFrame by date (index)\n",
    "df = df.sort_index()\n",
    "\n",
    "# 8. Select a specific column (e.g., 'Open' prices)\n",
    "open_prices = df['Open']\n",
    "print(\"\\n'Open' prices:\")\n",
    "print(open_prices.head())\n",
    "\n",
    "# 9. Select multiple columns at once (e.g., 'Close' and 'Volume')\n",
    "selected_columns = df[['Close', 'Volume']]\n",
    "print(\"\\n'Close' and 'Volume' columns:\")\n",
    "print(selected_columns.head())\n",
    "\n",
    "# 10. Filter data: select rows where the 'Close' price is above 50000\n",
    "filtered_df = df[df['Close'] > 50000]\n",
    "print(\"\\nRows with 'Close' price above 50000:\")\n",
    "print(filtered_df.head())\n",
    "\n",
    "# 11. Use loc to select data within a specific date range\n",
    "date_filtered_df = df.loc['2023-01-01':'2023-03-01']\n",
    "print(\"\\nData between 2023-01-01 and 2023-03-01:\")\n",
    "print(date_filtered_df.head())\n",
    "\n",
    "# 12. Use iloc to select the first 10 rows based on numerical indexing\n",
    "print(\"\\nFirst 10 rows using iloc:\")\n",
    "print(df.iloc[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging DataFrames (Merge/Join) \n",
    " is similar to join in SQL; that is, connecting rows based on one or more common keys between two DataFrames. The main function for this is pd.merge() (the df.merge() method is equivalent on an existing DataFrame).\n",
    "  Let's assume we have two DataFrames: \n",
    "  df_customers, which includes the columns CustomerID and Name (customer information), and df_orders, which includes the columns OrderID, CustomerID, and Amount (order information). We want to merge the information based on CustomerIDs to know which orders each customer has made. To do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df_customers, df_orders, on='CustomerID', how='inner')\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, on='CustomerID' specifies that the merge is to be performed based on the CustomerID column. The parameter how='inner' determines the type of join (an inner join brings only those customers that exist in both dataframes). You can also choose how as 'left', 'right', or 'outer': \n",
    "left: all rows from df_customers are preserved, and if there's no match in df_orders, null values come in. \n",
    "right: the opposite of the previous case. \n",
    "outer: the complete union, meaning all rows from both dataframes (null values if there's no match). The output merged_df includes a combination of columns from the two dataframes. For instance, the Name and CustomerID columns from customers and the OrderID and Amount columns from orders are placed side by side. If there are columns with the same name (other than the key) in both dataframes, Pandas adds suffixes like _x and _y to them to create distinction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenation means joining two datasets one after the other. This is done with the pd.concat() function. There are two modes: vertical concatenation (stacking rows on top of each other) and horizontal concatenation (placing columns side by side): for vertical concatenation (adding rows): the dataframes must have the same columns. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide = pd.concat([df_left, df_right], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, two dataframes with the same number of rows (or overlapping indices) are combined side by side with different columns, creating a wider dataframe.\n",
    "The concat function also has a keys parameter that is used if you need to create a multi-level index to distinguish the data sources. This is relevant for advanced users and is beyond the scope of this document, but you can find the details in the Pandas documentation. Now we are acquainted with the fundamentals of working with Pandas in reading, processing, filtering, grouping, sorting, and combining data. Next, we will move on to the data visualization section using the Matplotlib library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Visualization with Matplotlib\n",
    " After preparing and conducting preliminary data analysis with Pandas, the next step is usually the visualization of results and findings. Matplotlib is the main Python library for drawing various types of charts and graphs, which is particularly suitable for use in Jupyter Notebook. In this section, we will first set up Matplotlib and then create common types of charts.\n",
    "  To use Matplotlib, we must first import this library. The most common method is to import the pyplot submodule from Matplotlib with the alias plt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Jupyter Notebook environment, to ensure that charts are displayed within the notebook itself, this magical line is usually executed at the beginning of the notebook:\n",
    "%matplotlib inline\n",
    "\n",
    " This line displays the charts 'Inline' (within the same notebook page). (In newer versions of Jupyter, this behavior is enabled by default, but adding this line does no harm.) Now we can start drawing charts. Matplotlib supports various types of charts including line, bar, scatter, histogram, box plots, etc. Next, we will examine some common types of charts and how to plot them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
