{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas is a powerful and flexible open-source data analysis and manipulation library for Python. It provides data structures like DataFrames and Series, which are essential for handling structured data. With pandas, you can easily perform operations such as data cleaning, transformation, aggregation, and visualization. It is widely used in data science, machine learning, and statistical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd \n",
    "and read data and save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "df =yf.download()\n",
    "df =pd.read_csv(\"data.csv\")#read csv\n",
    "print(df.head())#show 5 line frist of the data\n",
    "df_excel=pd.read_excel('data.xlsx',sheet_name='sheet') #now we read sheet from data xlsx\n",
    "                             #how to save DataFrame?\n",
    "df.to_csv('output.csv',index=False) #save csv whithout index \n",
    "df.to_excel('output.xlsx',sheet_name='Results',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading data in Pandas, it is usually necessary to get an initial understanding of the content and structure of the DataFrame. Here are some useful methods for this purpose:\n",
    "\n",
    "df.head(n) and df.tail(n): Display the first or last n rows (if n is not specified, the default is 5). This provides a quick view of the data.\n",
    "df.info(): Displays a summary of the DataFrame, including the number of rows and columns, column names and data types, and the count of non-null values.\n",
    "df.shape: Returns the dimensions of the DataFrame as (number of rows, number of columns).\n",
    "df.describe(): Provides basic descriptive statistics for numerical columns (such as mean, standard deviation, minimum, maximum, and quartiles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the after read the df\n",
    "print(df.shape) #the exist 5,1000 5 column and 10000 line\n",
    "print(df.columns) #show title of colmun\n",
    "df.info() #show summary of the data\n",
    "print(df.head(3)) # show the 3 line of frist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of these commands provides general information about the DataFrame df. For example, df.info() is useful to determine the data type of each column (integer, float, string, etc.) and to check whether any columns contain missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.describe())\n",
    "# The df.describe() method provides a summary of the numerical columns in the DataFrame.\n",
    "# It includes the count, mean, standard deviation, minimum, 25th percentile, median (50th percentile), 75th percentile, and maximum values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting and Filtering Data\n",
    "Pandas provides powerful capabilities for selecting subsets of data (including specific rows and columns) and filtering based on logical conditions. Below are different methods for selecting data:\n",
    "\n",
    "Selecting Columns\n",
    "\n",
    "Using column names:\n",
    "df['ColumnName'] returns a Series containing the data of that column.\n",
    "To select multiple columns, pass a list of column names: df[['Col1', 'Col2']] returns a DataFrame containing only those columns.\n",
    "\n",
    "Using attributes:\n",
    "\n",
    "If the column name has no spaces or special characters, you can also use df.ColumnName. However, the previous method is more general and reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select one column \n",
    "ages=df['Age'] #The series related to the 'Age' column.\n",
    "\n",
    "#select some colmun\n",
    "\n",
    "subset=df[['Namw','City']] #The DataFrame includes two columns 'Name' and 'City'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting rows based on index using label-based and positional methods:\n",
    "\n",
    " df.loc[index_label] for selection based on the label or name of the row index (and also column), and df.iloc[index_position] for selection based on numerical position (row/column index numbers). \n",
    "\n",
    " To select a range of rows: df[start_index : end_index] (similar to slicing a list in Python) also works and returns rows from start to end-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "first_row=df.iloc[0] #The first row of the DataFrame as a Series.\n",
    "first_three_rows=df.iloc[0:3] #First three rows of the DataFrame.\n",
    "\n",
    "# If the dataframe has label-based indexing (for example, names of individuals) \n",
    "# df.loc['Ali'] returns the row with the label 'Ali' in the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering rows based on a condition\n",
    " is one of the most commonly used features of Pandas; it involves filtering rows based on logical conditions. This is done by creating a boolean mask (True/False) and applying it to the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adults=df[df['Age']>30] #filtering the age more than 30\n",
    "\n",
    "mask=(df['Age']>30)& (df['City'] == 'Tehran')\n",
    "\n",
    "filtered_df=df[mask]\n",
    "print(filtered_df)\n",
    "\n",
    "#Filter with multiple conditions: individuals who are older than 30 and their city is Tehran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, df['Age'] > 30 produces a Boolean Series that indicates for each row whether the condition of age being over 30 is met or not. Using & (logical AND) between two conditions allows for the combination of conditions (note: in Pandas, you should use & instead of and, and the sides of the conditions should be enclosed in parentheses). Finally, df[mask] returns the rows where the condition is True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method for filtering is to use the query method, which is written as a logical condition string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering with query (writing style similar to SQL)\n",
    "filtered_df=df.query(\"Age > and City =='Tehran'\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning and preparing data\n",
    " Real-world data often needs cleaning and preprocessing to be suitable for analysis. Pandas offers many functions for data cleaning, and in this section, we will refer to the most important ones: \n",
    "Missing Values: \n",
    "  Identifying and managing missing or empty data is very important. In Pandas, these values are typically represented as NaN (Not a Number). To find missing values, one can use df.isnull() or df.isna() (and their inverses df.notnull()), which return a boolean DataFrame.\n",
    " Removing missing values: \n",
    "  With df.dropna(), one can delete rows that have missing values. For example, df.dropna(subset=['Age', 'City']) will delete rows that have NaN in the Age or City columns. The parameter how='all' or how='any' can be used to remove rows where all their columns are empty or at least one of their columns is empty (the default is how='any').\n",
    " Replacingmissing values:\n",
    "  With df.fillna(), missing values can be filled with a fixed value or, for example, the mean of that column. Example: df['Age'].fillna(df['Age'].mean(), inplace=True) places the mean age in all empty locations of the Age column (the inplace=True parameter applies the change to the DataFrame itself).\n",
    " Removing duplicate data:\n",
    "  If there are duplicate rows, we use df.drop_duplicates(). This method can operate based on one or more specific columns. For example, df.drop_duplicates(subset=['Name', 'City'], keep='first') removes duplicate rows concerning the combination of Name and City (except for the first occurrence).\n",
    " Changing the data type of columns:\n",
    "  Sometimes it is necessary to change the data type of a column (for example, from string to number or vice versa). The astype method can be used. Example: df['Age'] = df['Age'].astype('int') changes the data type of the Age column to integer. If the conversion is inconsistent (for example, converting letters to numbers), an error occurs, which needs to be cleaned or managed before conversion.\n",
    " Changing column names:\n",
    "  The df.rename() method can be used to change column or index names. Example: df.rename(columns={'OldName': 'NewName'}, inplace=True). Additionally, the df.columns attribute is editable; for instance, df.columns = ['A', 'B', 'C'] changes the names of all columns at once. \n",
    "Applying functions to data:\n",
    " Pandas allows for applying a function to all values of a column or row with apply. For example, if we want to convert the price column, which is in dollars, to tomans (assuming the dollar is 30,000 tomans):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Price_Toman'] = df['Price'].apply(lambda x: x * 30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command adds a new column Price_Toman to the dataframe, which is the result of multiplying the Price value by 30000. The apply method is very flexible and can apply more complex functions (or even user-defined functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping and summarizing data is one of the key capabilities\n",
    " of Pandas, allowing for the grouping of data and then performing aggregations on each group. This functions similarly to the GROUP BY statement in SQL and is very useful for summarizing data based on a common attribute.\n",
    " To group data in Pandas, we use the groupby() method. This method creates a grouping based on one or more columns and returns a special object on which operations such as calculating the mean, sum, count, and so on can be performed.\n",
    "  Example: Suppose we have a DataFrame named sales_df that includes the 'City' and 'Revenue' columns. We want to calculate the total revenue based on each city:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped=sales_df.groupby('City') #grouped by City column\n",
    "result=grouped['Revenue'].sum() # Calculate the total income for each city\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, sales_df.groupby('City') groups the data based on the values in the City column. Then, we selected the 'Revenue' column on the grouped object and applied the sum() method, resulting in a new Series containing the total revenue for each unique City value. The output of print(result) may look something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "City\n",
    "Tehran     1500000  \n",
    "Shiraz      800000  \n",
    "Mashhad     620000  \n",
    "...            ...  \n",
    "Name: Revenue, dtype: int64\n",
    "\n",
    " In the above example, we have the total Revenue for each city. Similarly, one can use other functions such as mean, count, max, min, etc. Moreover, multiple functions can be applied simultaneously using the agg method:\n",
    "It is also possible to use multiple functions simultaneously with the agg method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = grouped['Revenue'].agg(['count','mean','sum'])\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code returns a dataframe that shows the number of rows, average income, and total income for each city.\n",
    " To group by multiple columns, we simply need to pass the list of columns to groupby. For example: df.groupby(['Region', 'City']).sum() groups the data first by Region and then by City, providing the sum for each combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting data:\n",
    " Sorting (Sorting) data either by the values of the columns or by the index is easily done in Pandas: df.sort_values(by='ColumnName'):\n",
    "  sorts the DataFrame in ascending order based on the specified column value (ascending is the default). To sort in descending order, we use ascending=False. You can also provide by as a list of columns to sort based on priority. df.sort_index(): \n",
    "  sorts the DataFrame by index. To sort the index in reverse, ascending=False can be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting by a column in ascending order\n",
    "sorted_df=df.sort_values(by='Age')\n",
    "\n",
    "#Sorting by a column in descending order\n",
    "sorted_df2=df.sort_values(by='Revenue',ascending=False)\n",
    "\n",
    "#Sorting by multiple columns (for example, first alphabetically by City,\n",
    "#and in case of a tie, by Age in ascending order)\n",
    "sorted_df3=df.sort_values(by=['City','Age'],ascending=[True,True])\n",
    "\n",
    "#Sorting by index of lines\n",
    "df_sorted_index=df.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After sorting, the sorted dataframe (sorted_df, etc.) can be viewed or used. If we want the sorting change to be applied to the same dataframe, we can add inplace=True to the method (for example, df.sort_values(..., inplace=True)). However, please note that in this case, the original dataframe will be modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data projects, it is often necessary to combine information from multiple data sources. The Pandas library has two main methods for combining data: merging and concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries if not already installed\n",
    "# !pip install yfinance\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Download Bitcoin data from Yahoo Finance\n",
    "df = yf.download(\"BTC-USD\", start=\"2020-01-01\", end=\"2021-03-01\")\n",
    "\n",
    "# 2. Display the first few and last few rows of the DataFrame\n",
    "print(\"First few rows (df.head()):\")\n",
    "print(df.head())  # By default, displays 5 rows\n",
    "\n",
    "print(\"\\nLast few rows (df.tail()):\")\n",
    "print(df.tail())  # By default, displays 5 rows\n",
    "\n",
    "# 3. Display DataFrame summary information\n",
    "print(\"\\nDataFrame information (df.info()):\")\n",
    "df.info()\n",
    "\n",
    "# 4. Display the shape of the DataFrame (number of rows and columns)\n",
    "print(\"\\nDataFrame shape (df.shape):\")\n",
    "print(df.shape)\n",
    "\n",
    "# 5. Display descriptive statistics for numerical columns\n",
    "print(\"\\nDescriptive statistics (df.describe()):\")\n",
    "print(df.describe())\n",
    "\n",
    "# 6. Remove duplicate rows and rows with missing values\n",
    "df = df.drop_duplicates()  # Remove duplicate rows\n",
    "df = df.dropna()           # Remove rows with missing (NaN) values\n",
    "\n",
    "# 7. Sort the DataFrame by date (index)\n",
    "df = df.sort_index()\n",
    "\n",
    "# 8. Select a specific column (e.g., 'Open' prices)\n",
    "open_prices = df['Open']\n",
    "print(\"\\n'Open' prices:\")\n",
    "print(open_prices.head())\n",
    "\n",
    "# 9. Select multiple columns at once (e.g., 'Close' and 'Volume')\n",
    "selected_columns = df[['Close', 'Volume']]\n",
    "print(\"\\n'Close' and 'Volume' columns:\")\n",
    "print(selected_columns.head())\n",
    "\n",
    "# 10. Filter data: select rows where the 'Close' price is above 50000\n",
    "filtered_df = df[df['Close'] > 50000]\n",
    "print(\"\\nRows with 'Close' price above 50000:\")\n",
    "print(filtered_df.head())\n",
    "\n",
    "# 11. Use loc to select data within a specific date range\n",
    "date_filtered_df = df.loc['2023-01-01':'2023-03-01']\n",
    "print(\"\\nData between 2023-01-01 and 2023-03-01:\")\n",
    "print(date_filtered_df.head())\n",
    "\n",
    "# 12. Use iloc to select the first 10 rows based on numerical indexing\n",
    "print(\"\\nFirst 10 rows using iloc:\")\n",
    "print(df.iloc[:10])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
